{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h1 style=\"color:#4cd5e5;font-size:70px;font-family:Georgia;text-align:center;\"><strong>WELCOME <strong\nstyle=\"color:#f8de31;font-size:60px;font-family:Georgia;\">TO <strong style=\"color:#4cd5e5;font-size:70px;font-\nfamily:Georgia;\">NLP <strong style=\"color:#f8de31;font-size:60px;font-family:Georgia;\">-Language <strong\nstyle=\"color:#4cd5e5;font-size:70px;font-family:Georgia;\">Detector <strong style=\"color:#4cd5e5;font-size:60px;font\n-family:Georgia;\">:- </strong></strong></strong></strong></strong></strong></h1>\n","metadata":{}},{"cell_type":"markdown","source":"<header><h2><p style=\"background-color:#FFFFF;font-family:Georgia;color:#000000;font-size:90%;text-align:center;border-radius:10px 10px;border-style: dotted;border-width:5px;border-color:#000000;\"><b>Information is the oil of the 21st century, and analytics is the combustion engine.</b></p></h2></header>\n<p style= \"background-color:#e9fafc;font-family:Georgia;color:#000000;font-size:60%;text-align:center;border-radius:10px 10px\"></p>","metadata":{"execution":{"iopub.status.busy":"2022-02-17T05:45:02.017273Z","iopub.execute_input":"2022-02-17T05:45:02.017617Z","iopub.status.idle":"2022-02-17T05:45:02.024442Z","shell.execute_reply.started":"2022-02-17T05:45:02.017580Z","shell.execute_reply":"2022-02-17T05:45:02.023324Z"}}},{"cell_type":"markdown","source":"<h1 style=\"color:#4cd5e5;font-size:60px;font-family:Georgia;text-align:center;\"><strong>What <strong style=\"color:#f8de31;font-size:50px;font-family:Georgia;\">We <strong style=\"color:#4cd5e5;font-size:60px;font-family:Georgia;\">Are <strong style=\"color:#f8de31;font-size:50px;font-family:Georgia;\">Doing <strong style=\"color:#4cd5e5;font-size:60px;font-family:Georgia;\">In <strong style=\"color:#4cd5e5;font-size:50px;font-family:Georgia;\">This <strong style=\"color:#4cd5e5;font-size:60px;font-family:Georgia;\">Kernel <strong style=\"color:#4cd5e5;font-size:50px;font-family:Georgia;\">? </strong></strong></strong></strong></strong></strong></strong></strong></h1>\n\n\n<p style= \"background-color:#e9fafc;font-family:Georgia;color:#000000;font-size:80%;text-align:center;border-radius:10px 10px\"><b>--Understanding the problem.<br>-About the dataset.<br>-Examine the dataset.<br>--General:-<br>\n    <br>\n   1.First of 5 rows.<br>\n   2.Number of rows and columns.<br>\n   3.Column name.<br>\n   4.Value count.<br> \n   5.Separating Independent and Dependent features<br>\n   6.Label Encoding<br>\n   7.Text Preprocessing<br>\n   8.Bag of Words<br>\n   9.Train Test Splitting<br>\n   10.Model Training and Prediction<br>\n   11.Model Evaluation<br>Predicting with some more data</b></p>","metadata":{}},{"cell_type":"markdown","source":"<p style= \"background-color:#000000;font-family:Georgia;color:#FFFFFF;font-size:125%;text-align:center;border-radius:10px 10px;border-style:solid;border-width:3px;border-color:#000000;\"><b>Importing libraries and dataset:-</b></p>","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport re\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nimport sklearn\nwarnings.simplefilter(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2022-02-17T05:25:51.262508Z","iopub.execute_input":"2022-02-17T05:25:51.262957Z","iopub.status.idle":"2022-02-17T05:25:51.268156Z","shell.execute_reply.started":"2022-02-17T05:25:51.262921Z","shell.execute_reply":"2022-02-17T05:25:51.267147Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"<p style= \"background-color:#000000;font-family:Georgia;color:#FFFFFF;font-size:125%;text-align:center;border-radius:10px 10px;border-style:solid;border-width:3px;border-color:#000000;\"><b>Import the language detection dataset-</b></p","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv(\"../input/language-detection/Language Detection.csv\")\ndata.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-02-17T05:25:51.270154Z","iopub.execute_input":"2022-02-17T05:25:51.270495Z","iopub.status.idle":"2022-02-17T05:25:51.379682Z","shell.execute_reply.started":"2022-02-17T05:25:51.270448Z","shell.execute_reply":"2022-02-17T05:25:51.378744Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"<p style= \"background-color:#e9fafc;font-family:Georgia;color:#000000;font-size:100%;text-align:center;border-radius:10px 10px\"><b>As we know this dataset contains text details for 17 different languages. So let’s count the value count for each language.</b></p>","metadata":{}},{"cell_type":"code","source":"data[\"Language\"].value_counts()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-02-17T05:25:51.381238Z","iopub.execute_input":"2022-02-17T05:25:51.381436Z","iopub.status.idle":"2022-02-17T05:25:51.398163Z","shell.execute_reply.started":"2022-02-17T05:25:51.381411Z","shell.execute_reply":"2022-02-17T05:25:51.397279Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"<p style= \"background-color:#000000;font-family:Georgia;color:#FFFFFF;font-size:125%;text-align:center;border-radius:10px 10px;border-style:solid;border-width:3px;border-color:#000000;\"><b>Separating Independent and Dependent features-</b></p","metadata":{}},{"cell_type":"code","source":"X = data[\"Text\"]\ny = data[\"Language\"]","metadata":{"execution":{"iopub.status.busy":"2022-02-17T05:25:51.400616Z","iopub.execute_input":"2022-02-17T05:25:51.400871Z","iopub.status.idle":"2022-02-17T05:25:51.406880Z","shell.execute_reply.started":"2022-02-17T05:25:51.400842Z","shell.execute_reply":"2022-02-17T05:25:51.405799Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"<p style= \"background-color:#000000;font-family:Georgia;color:#FFFFFF;font-size:125%;text-align:center;border-radius:10px 10px;border-style:solid;border-width:3px;border-color:#000000;\"><b>Label Encoding</b></p","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ny = le.fit_transform(y)","metadata":{"execution":{"iopub.status.busy":"2022-02-17T05:25:51.408586Z","iopub.execute_input":"2022-02-17T05:25:51.409051Z","iopub.status.idle":"2022-02-17T05:25:51.452147Z","shell.execute_reply.started":"2022-02-17T05:25:51.409003Z","shell.execute_reply":"2022-02-17T05:25:51.451257Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"<p style= \"background-color:#000000;font-family:Georgia;color:#FFFFFF;font-size:125%;text-align:center;border-radius:10px 10px;border-style:solid;border-width:3px;border-color:#000000;\"><b>Text Preprocessing</b></p\n","metadata":{}},{"cell_type":"markdown","source":"<p style= \"background-color:#e9fafc;font-family:Georgia;color:#000000;font-size:100%;text-align:center;border-radius:10px 10px\"><b>This is a dataset created using scraping the Wikipedia, so it contains many unwanted symbols, numbers which will affect the quality of our model. So we should perform text preprocessing techniques.</b></p>","metadata":{}},{"cell_type":"code","source":"# creating a list for appending the preprocessed text\ndata_list = []\n# iterating through all the text\nfor text in X:\n       # removing the symbols and numbers\n        text = re.sub(r'[!@#$(),n\"%^*?:;~`0-9]', ' ', text)\n        text = re.sub(r'[[]]', ' ', text)\n        # converting the text to lower case\n        text = text.lower()\n        # appending to data_list\n        data_list.append(text)","metadata":{"execution":{"iopub.status.busy":"2022-02-17T05:25:51.453450Z","iopub.execute_input":"2022-02-17T05:25:51.453667Z","iopub.status.idle":"2022-02-17T05:25:51.559663Z","shell.execute_reply.started":"2022-02-17T05:25:51.453641Z","shell.execute_reply":"2022-02-17T05:25:51.558563Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"<p style= \"background-color:#000000;font-family:Georgia;color:#FFFFFF;font-size:125%;text-align:center;border-radius:10px 10px;border-style:solid;border-width:3px;border-color:#000000;\"><b>Bag of Words</b></p","metadata":{}},{"cell_type":"markdown","source":"<p style= \"background-color:#e9fafc;font-family:Georgia;color:#000000;font-size:100%;text-align:center;border-radius:10px 10px\"><b>As we know the output feature but also the input feature should be of the numerical form. So we are converting text into numerical form by creating a Bag of Words model using CountVectorizer.</b></p>","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\ncv = CountVectorizer()\nX = cv.fit_transform(data_list).toarray()\nX.shape # (10337, 39419)","metadata":{"execution":{"iopub.status.busy":"2022-02-17T05:25:51.560756Z","iopub.execute_input":"2022-02-17T05:25:51.560989Z","iopub.status.idle":"2022-02-17T05:25:52.276506Z","shell.execute_reply.started":"2022-02-17T05:25:51.560963Z","shell.execute_reply":"2022-02-17T05:25:52.275309Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"<p style= \"background-color:#000000;font-family:Georgia;color:#FFFFFF;font-size:125%;text-align:center;border-radius:10px 10px;border-style:solid;border-width:3px;border-color:#000000;\"><b>Train Test Splitting</b></p ","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.20)","metadata":{"execution":{"iopub.status.busy":"2022-02-17T05:25:52.277489Z","iopub.execute_input":"2022-02-17T05:25:52.277718Z","iopub.status.idle":"2022-02-17T05:25:54.186944Z","shell.execute_reply.started":"2022-02-17T05:25:52.277692Z","shell.execute_reply":"2022-02-17T05:25:54.185951Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"<p style= \"background-color:#000000;font-family:Georgia;color:#FFFFFF;font-size:125%;text-align:center;border-radius:10px 10px;border-style:solid;border-width:3px;border-color:#000000;\"><b>Model Training and Prediction</b></p> ","metadata":{}},{"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\nmodel = MultinomialNB()\nmodel.fit(x_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-02-17T05:25:54.188038Z","iopub.execute_input":"2022-02-17T05:25:54.188260Z","iopub.status.idle":"2022-02-17T05:26:25.799949Z","shell.execute_reply.started":"2022-02-17T05:25:54.188234Z","shell.execute_reply":"2022-02-17T05:26:25.799002Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"y_pred = model.predict(x_test)","metadata":{"execution":{"iopub.status.busy":"2022-02-17T05:26:25.802358Z","iopub.execute_input":"2022-02-17T05:26:25.802591Z","iopub.status.idle":"2022-02-17T05:26:26.238098Z","shell.execute_reply.started":"2022-02-17T05:26:25.802565Z","shell.execute_reply":"2022-02-17T05:26:26.237072Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"<p style= \"background-color:#000000;font-family:Georgia;color:#FFFFFF;font-size:125%;text-align:center;border-radius:10px 10px;border-style:solid;border-width:3px;border-color:#000000;\"><b>Model Evaluation</b></p> ","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nac = accuracy_score(y_test, y_pred)\ncm = confusion_matrix(y_test, y_pred)\n\nprint(\"Accuracy is :\",ac)\n","metadata":{"execution":{"iopub.status.busy":"2022-02-17T05:26:26.244408Z","iopub.execute_input":"2022-02-17T05:26:26.245203Z","iopub.status.idle":"2022-02-17T05:26:26.261955Z","shell.execute_reply.started":"2022-02-17T05:26:26.245144Z","shell.execute_reply":"2022-02-17T05:26:26.261052Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"<p style= \"background-color:#e9fafc;font-family:Georgia;color:#000000;font-size:100%;text-align:center;border-radius:10px 10px\"><b>The accuracy of the model is 0.97 which is very good and our model is performing well. Now let’s plot the confusion matrix using the seaborn heatmap.</b></p>","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(15,10))\nsns.heatmap(cm, annot = True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-17T05:26:26.263595Z","iopub.execute_input":"2022-02-17T05:26:26.264135Z","iopub.status.idle":"2022-02-17T05:26:27.531496Z","shell.execute_reply.started":"2022-02-17T05:26:26.264090Z","shell.execute_reply":"2022-02-17T05:26:27.530608Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"<p style= \"background-color:#e9fafc;font-family:Georgia;color:#000000;font-size:100%;text-align:center;border-radius:10px 10px\"><b>Almost all the predictions are right.So lets Continue</b></p>","metadata":{}},{"cell_type":"markdown","source":"<p style= \"background-color:#000000;font-family:Georgia;color:#FFFFFF;font-size:125%;text-align:center;border-radius:10px 10px;border-style:solid;border-width:3px;border-color:#000000;\"><b>Predicting with some more data</b></p> ","metadata":{}},{"cell_type":"code","source":"def predict():\n     text = str(input())   \n     x = cv.transform([text]).toarray() # converting text to bag of words model (Vector)\n     lang = model.predict(x) # predicting the language\n     lang = le.inverse_transform(lang) # finding the language corresponding the the predicted value\n     print(\"The langauge is in\",lang[0]) # printing the language","metadata":{"execution":{"iopub.status.busy":"2022-02-17T05:26:27.533086Z","iopub.execute_input":"2022-02-17T05:26:27.534037Z","iopub.status.idle":"2022-02-17T05:26:27.541191Z","shell.execute_reply.started":"2022-02-17T05:26:27.533987Z","shell.execute_reply":"2022-02-17T05:26:27.540240Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"predict()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-02-17T05:26:27.542698Z","iopub.execute_input":"2022-02-17T05:26:27.543275Z","iopub.status.idle":"2022-02-17T05:31:03.357801Z","shell.execute_reply.started":"2022-02-17T05:26:27.543192Z","shell.execute_reply":"2022-02-17T05:31:03.356840Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"import pickle","metadata":{"execution":{"iopub.status.busy":"2022-02-17T05:31:03.359904Z","iopub.execute_input":"2022-02-17T05:31:03.360528Z","iopub.status.idle":"2022-02-17T05:31:03.365285Z","shell.execute_reply.started":"2022-02-17T05:31:03.360479Z","shell.execute_reply":"2022-02-17T05:31:03.364184Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"pickle_out= open(\"language1.pkl\",\"wb\")\npickle.dump(model,pickle_out)\npickle_out.close()","metadata":{"execution":{"iopub.status.busy":"2022-02-17T05:31:03.367201Z","iopub.execute_input":"2022-02-17T05:31:03.367726Z","iopub.status.idle":"2022-02-17T05:31:03.404949Z","shell.execute_reply.started":"2022-02-17T05:31:03.367683Z","shell.execute_reply":"2022-02-17T05:31:03.403894Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}